{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T07:29:43.841341Z",
     "iopub.status.busy": "2023-02-03T07:29:43.840427Z",
     "iopub.status.idle": "2023-02-03T07:29:45.916636Z",
     "shell.execute_reply": "2023-02-03T07:29:45.915790Z",
     "shell.execute_reply.started": "2023-02-03T07:29:43.841309Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from paddle import nn\n",
    "from PIL import Image\n",
    "from paddle.distributed import fleet, get_rank\n",
    "\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 4\n",
    "EPOCH_NUM = 5\n",
    "\n",
    "print(paddle.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T07:31:29.272547Z",
     "iopub.status.busy": "2023-02-03T07:31:29.271950Z",
     "iopub.status.idle": "2023-02-03T07:31:29.283702Z",
     "shell.execute_reply": "2023-02-03T07:31:29.282943Z",
     "shell.execute_reply.started": "2023-02-03T07:31:29.272513Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义数据集\n",
    "class MyDataset(paddle.io.Dataset):\n",
    "    def __init__(self, img_dir='data/PALM-Training400/', csv_dir='data/Classification.csv') -> None:\n",
    "        super(MyDataset, self).__init__()\n",
    "        if csv_dir is None:\n",
    "            self.csvfile = None\n",
    "            self.filedir = os.listdir(img_dir)\n",
    "        else:\n",
    "            self.csvfile = pd.read_csv(csv_dir)\n",
    "        self.imgpath = img_dir\n",
    "        pass\n",
    "    def __len__(self):\n",
    "        if self.csvfile is None:\n",
    "            return len(self.filedir)\n",
    "        else:\n",
    "            return len(self.csvfile)\n",
    "        pass\n",
    "    def __getitem__(self, idx):\n",
    "        if self.csvfile is None:\n",
    "            img = np.reshape((np.array(Image.open(self.imgpath+os.sep+self.filedir[idx]).resize((IMAGE_SIZE,IMAGE_SIZE))).astype('float32')),(3,IMAGE_SIZE,IMAGE_SIZE))/256.\n",
    "            lab = self.filedir[idx]\n",
    "        else:\n",
    "            img = np.reshape((np.array(Image.open(self.imgpath+os.sep+self.csvfile['imgName'][idx]).resize((IMAGE_SIZE,IMAGE_SIZE))).astype('float32')),(3,IMAGE_SIZE,IMAGE_SIZE))/256.\n",
    "            lab = np.array(self.csvfile['Label'][idx]).astype('float32')\n",
    "        return img,lab\n",
    "    pass\n",
    "mydataset = MyDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T07:29:47.007713Z",
     "iopub.status.busy": "2023-02-03T07:29:47.006744Z",
     "iopub.status.idle": "2023-02-03T07:29:48.508882Z",
     "shell.execute_reply": "2023-02-03T07:29:48.508026Z",
     "shell.execute_reply.started": "2023-02-03T07:29:47.007677Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ResNet模型代码 https://www.paddlepaddle.org.cn/tutorials/projectdetail/4464926#anchor-9\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "# ResNet中使用了BatchNorm层，在卷积层的后面加上BatchNorm以提升数值稳定性\n",
    "# 定义卷积批归一化块\n",
    "class ConvBNLayer(paddle.nn.Layer):\n",
    "    def __init__(self,\n",
    "                 num_channels,\n",
    "                 num_filters,\n",
    "                 filter_size,\n",
    "                 stride=1,\n",
    "                 groups=1,\n",
    "                 act=None):\n",
    "       \n",
    "        \"\"\"\n",
    "        num_channels, 卷积层的输入通道数\n",
    "        num_filters, 卷积层的输出通道数\n",
    "        stride, 卷积层的步幅\n",
    "        groups, 分组卷积的组数，默认groups=1不使用分组卷积\n",
    "        \"\"\"\n",
    "        super(ConvBNLayer, self).__init__()\n",
    "\n",
    "        # 创建卷积层\n",
    "        self._conv = nn.Conv2D(\n",
    "            in_channels=num_channels,\n",
    "            out_channels=num_filters,\n",
    "            kernel_size=filter_size,\n",
    "            stride=stride,\n",
    "            padding=(filter_size - 1) // 2,\n",
    "            groups=groups,\n",
    "            bias_attr=False)\n",
    "\n",
    "        # 创建BatchNorm层\n",
    "        self._batch_norm = paddle.nn.BatchNorm2D(num_filters)\n",
    "        \n",
    "        self.act = act\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        y = self._conv(inputs)\n",
    "        y = self._batch_norm(y)\n",
    "        if self.act == 'leaky':\n",
    "            y = F.leaky_relu(x=y, negative_slope=0.1)\n",
    "        elif self.act == 'relu':\n",
    "            y = F.relu(x=y)\n",
    "        return y\n",
    "\n",
    "# 定义残差块\n",
    "# 每个残差块会对输入图片做三次卷积，然后跟输入图片进行短接\n",
    "# 如果残差块中第三次卷积输出特征图的形状与输入不一致，则对输入图片做1x1卷积，将其输出形状调整成一致\n",
    "class BottleneckBlock(paddle.nn.Layer):\n",
    "    def __init__(self,\n",
    "                 num_channels,\n",
    "                 num_filters,\n",
    "                 stride,\n",
    "                 shortcut=True):\n",
    "        super(BottleneckBlock, self).__init__()\n",
    "        # 创建第一个卷积层 1x1\n",
    "        self.conv0 = ConvBNLayer(\n",
    "            num_channels=num_channels,\n",
    "            num_filters=num_filters,\n",
    "            filter_size=1,\n",
    "            act='relu')\n",
    "        # 创建第二个卷积层 3x3\n",
    "        self.conv1 = ConvBNLayer(\n",
    "            num_channels=num_filters,\n",
    "            num_filters=num_filters,\n",
    "            filter_size=3,\n",
    "            stride=stride,\n",
    "            act='relu')\n",
    "        # 创建第三个卷积 1x1，但输出通道数乘以4\n",
    "        self.conv2 = ConvBNLayer(\n",
    "            num_channels=num_filters,\n",
    "            num_filters=num_filters * 4,\n",
    "            filter_size=1,\n",
    "            act=None)\n",
    "\n",
    "        # 如果conv2的输出跟此残差块的输入数据形状一致，则shortcut=True\n",
    "        # 否则shortcut = False，添加1个1x1的卷积作用在输入数据上，使其形状变成跟conv2一致\n",
    "        if not shortcut:\n",
    "            self.short = ConvBNLayer(\n",
    "                num_channels=num_channels,\n",
    "                num_filters=num_filters * 4,\n",
    "                filter_size=1,\n",
    "                stride=stride)\n",
    "\n",
    "        self.shortcut = shortcut\n",
    "\n",
    "        self._num_channels_out = num_filters * 4\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        y = self.conv0(inputs)\n",
    "        conv1 = self.conv1(y)\n",
    "        conv2 = self.conv2(conv1)\n",
    "\n",
    "        # 如果shortcut=True，直接将inputs跟conv2的输出相加\n",
    "        # 否则需要对inputs进行一次卷积，将形状调整成跟conv2输出一致\n",
    "        if self.shortcut:\n",
    "            short = inputs\n",
    "        else:\n",
    "            short = self.short(inputs)\n",
    "\n",
    "        y = paddle.add(x=short, y=conv2)\n",
    "        y = F.relu(y)\n",
    "        return y\n",
    "\n",
    "# 定义ResNet模型\n",
    "class ResNet(paddle.nn.Layer):\n",
    "    def __init__(self, layers=50, class_dim=1):\n",
    "        \"\"\"\n",
    "        \n",
    "        layers, 网络层数，可以是50, 101或者152\n",
    "        class_dim，分类标签的类别数\n",
    "        \"\"\"\n",
    "        super(ResNet, self).__init__()\n",
    "        self.layers = layers\n",
    "        supported_layers = [50, 101, 152]\n",
    "        assert layers in supported_layers, \\\n",
    "            \"supported layers are {} but input layer is {}\".format(supported_layers, layers)\n",
    "\n",
    "        if layers == 50:\n",
    "            #ResNet50包含多个模块，其中第2到第5个模块分别包含3、4、6、3个残差块\n",
    "            depth = [3, 4, 6, 3]\n",
    "        elif layers == 101:\n",
    "            #ResNet101包含多个模块，其中第2到第5个模块分别包含3、4、23、3个残差块\n",
    "            depth = [3, 4, 23, 3]\n",
    "        elif layers == 152:\n",
    "            #ResNet152包含多个模块，其中第2到第5个模块分别包含3、8、36、3个残差块\n",
    "            depth = [3, 8, 36, 3]\n",
    "        \n",
    "        # 残差块中使用到的卷积的输出通道数\n",
    "        num_filters = [64, 128, 256, 512]\n",
    "\n",
    "        # ResNet的第一个模块，包含1个7x7卷积，后面跟着1个最大池化层\n",
    "        self.conv = ConvBNLayer(\n",
    "            num_channels=3,\n",
    "            num_filters=64,\n",
    "            filter_size=7,\n",
    "            stride=2,\n",
    "            act='relu')\n",
    "        self.pool2d_max = nn.MaxPool2D(\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=1)\n",
    "\n",
    "        # ResNet的第二到第五个模块c2、c3、c4、c5\n",
    "        self.bottleneck_block_list = []\n",
    "        num_channels = 64\n",
    "        for block in range(len(depth)):\n",
    "            shortcut = False\n",
    "            for i in range(depth[block]):\n",
    "                # c3、c4、c5将会在第一个残差块使用stride=2；其余所有残差块stride=1\n",
    "                bottleneck_block = self.add_sublayer(\n",
    "                    'bb_%d_%d' % (block, i),\n",
    "                    BottleneckBlock(\n",
    "                        num_channels=num_channels,\n",
    "                        num_filters=num_filters[block],\n",
    "                        stride=2 if i == 0 and block != 0 else 1, \n",
    "                        shortcut=shortcut))\n",
    "                num_channels = bottleneck_block._num_channels_out\n",
    "                self.bottleneck_block_list.append(bottleneck_block)\n",
    "                shortcut = True\n",
    "\n",
    "        # 在c5的输出特征图上使用全局池化\n",
    "        self.pool2d_avg = paddle.nn.AdaptiveAvgPool2D(output_size=1)\n",
    "\n",
    "        # stdv用来作为全连接层随机初始化参数的方差\n",
    "        import math\n",
    "        stdv = 1.0 / math.sqrt(2048 * 1.0)\n",
    "        \n",
    "        # 创建全连接层，输出大小为类别数目，经过残差网络的卷积和全局池化后，\n",
    "        # 卷积特征的维度是[B,2048,1,1]，故最后一层全连接的输入维度是2048\n",
    "        self.out = nn.Linear(in_features=2048, out_features=class_dim,\n",
    "                      weight_attr=paddle.ParamAttr(\n",
    "                          initializer=paddle.nn.initializer.Uniform(-stdv, stdv)))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        y = self.conv(inputs)\n",
    "        y = self.pool2d_max(y)\n",
    "        for bottleneck_block in self.bottleneck_block_list:\n",
    "            y = bottleneck_block(y)\n",
    "        y = self.pool2d_avg(y)\n",
    "        y = paddle.reshape(y, [y.shape[0], -1])\n",
    "        y = self.out(y)\n",
    "        return y\n",
    "\n",
    "model = ResNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T06:55:36.679232Z",
     "iopub.status.busy": "2023-02-03T06:55:36.678631Z",
     "iopub.status.idle": "2023-02-03T06:55:36.691230Z",
     "shell.execute_reply": "2023-02-03T06:55:36.690326Z",
     "shell.execute_reply.started": "2023-02-03T06:55:36.679188Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_pm(model, optimizer):\n",
    "    # 开启0号GPU训练\n",
    "    paddle.device.set_device('gpu:0')\n",
    "\n",
    "    print('start training ... ')\n",
    "    model.train()\n",
    "    # 定义数据读取器，训练数据读取器和验证数据读取器\n",
    "    train_loader = paddle.io.DataLoader(mydataset,batch_size=BATCH_SIZE,shuffle=True,drop_last=True)\n",
    "    valid_loader = paddle.io.DataLoader(mydataset,batch_size=BATCH_SIZE,shuffle=True,drop_last=True)\n",
    "    for epoch in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            x_data, y_data = data\n",
    "            img = paddle.to_tensor(x_data)\n",
    "            label = paddle.reshape(paddle.to_tensor(y_data),(-1,1))\n",
    "            # 运行模型前向计算，得到预测值\n",
    "            logits = model(img)\n",
    "            loss = F.binary_cross_entropy_with_logits(logits, label)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "\n",
    "            if batch_id % 20 == 19:\n",
    "                print(\"epoch: {}, batch_id: {}, loss is: {:.4f}\".format(epoch+1, batch_id+1, float(avg_loss.numpy())))\n",
    "            # 反向传播，更新权重，清除梯度\n",
    "            avg_loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.clear_grad()\n",
    "\n",
    "        model.eval()\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        for batch_id, data in enumerate(valid_loader()):\n",
    "            x_data, y_data = data\n",
    "            img = paddle.to_tensor(x_data)\n",
    "            label = paddle.reshape(paddle.to_tensor(y_data),(-1,1))\n",
    "            # 运行模型前向计算，得到预测值\n",
    "            logits = model(img)\n",
    "            # 二分类，sigmoid计算后的结果以0.5为阈值分两个类别\n",
    "            # 计算sigmoid后的预测概率，进行loss计算\n",
    "            pred = F.sigmoid(logits)\n",
    "            loss = F.binary_cross_entropy_with_logits(logits, label)\n",
    "            # 计算预测概率小于0.5的类别\n",
    "            pred2 = pred * (-1.0) + 1.0\n",
    "            # 得到两个类别的预测概率，并沿第一个维度级联\n",
    "            pred = paddle.concat([pred2, pred], axis=1)\n",
    "            acc = paddle.metric.accuracy(pred, paddle.cast(label, dtype='int64'))\n",
    "\n",
    "            accuracies.append(acc.numpy())\n",
    "            losses.append(loss.numpy())\n",
    "        print(\"[validation] accuracy/loss: {:.4f}/{:.4f}\".format(np.mean(accuracies), np.mean(losses)))\n",
    "        model.train()\n",
    "\n",
    "        paddle.save(model.state_dict(), 'model/palmp.pdparams')\n",
    "        paddle.save(optimizer.state_dict(), 'model/palmp.pdopt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-02-03T06:55:36.694649Z",
     "iopub.status.busy": "2023-02-03T06:55:36.694177Z",
     "iopub.status.idle": "2023-02-03T07:13:43.621110Z",
     "shell.execute_reply": "2023-02-03T07:13:43.616991Z",
     "shell.execute_reply.started": "2023-02-03T06:55:36.694606Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training ... \n",
      "epoch: 1, batch_id: 20, loss is: 0.6849\n",
      "epoch: 1, batch_id: 40, loss is: 0.1636\n",
      "epoch: 1, batch_id: 60, loss is: 0.2042\n",
      "epoch: 1, batch_id: 80, loss is: 0.5876\n",
      "epoch: 1, batch_id: 100, loss is: 0.6481\n",
      "epoch: 1, batch_id: 120, loss is: 0.2350\n",
      "epoch: 1, batch_id: 140, loss is: 0.0544\n",
      "epoch: 1, batch_id: 160, loss is: 1.0782\n",
      "epoch: 1, batch_id: 180, loss is: 0.1324\n",
      "epoch: 1, batch_id: 200, loss is: 0.0660\n",
      "[validation] accuracy/loss: 0.9212/0.2150\n",
      "epoch: 2, batch_id: 20, loss is: 0.2800\n",
      "epoch: 2, batch_id: 40, loss is: 0.1168\n",
      "epoch: 2, batch_id: 60, loss is: 0.1659\n",
      "epoch: 2, batch_id: 80, loss is: 0.0434\n",
      "epoch: 2, batch_id: 100, loss is: 0.3766\n",
      "epoch: 2, batch_id: 120, loss is: 0.0184\n",
      "epoch: 2, batch_id: 140, loss is: 0.1121\n",
      "epoch: 2, batch_id: 160, loss is: 0.2832\n",
      "epoch: 2, batch_id: 180, loss is: 1.2500\n",
      "epoch: 2, batch_id: 200, loss is: 0.1320\n",
      "[validation] accuracy/loss: 0.8537/0.3332\n",
      "epoch: 3, batch_id: 20, loss is: 0.3118\n",
      "epoch: 3, batch_id: 40, loss is: 0.7205\n",
      "epoch: 3, batch_id: 60, loss is: 0.0311\n",
      "epoch: 3, batch_id: 80, loss is: 0.1347\n",
      "epoch: 3, batch_id: 100, loss is: 0.6658\n",
      "epoch: 3, batch_id: 120, loss is: 0.2840\n",
      "epoch: 3, batch_id: 140, loss is: 0.1387\n",
      "epoch: 3, batch_id: 160, loss is: 0.7540\n",
      "epoch: 3, batch_id: 180, loss is: 0.0917\n",
      "epoch: 3, batch_id: 200, loss is: 0.0432\n",
      "[validation] accuracy/loss: 0.9112/0.2676\n",
      "epoch: 4, batch_id: 20, loss is: 1.1730\n",
      "epoch: 4, batch_id: 40, loss is: 0.1012\n",
      "epoch: 4, batch_id: 60, loss is: 0.6688\n",
      "epoch: 4, batch_id: 80, loss is: 0.2123\n",
      "epoch: 4, batch_id: 100, loss is: 0.0644\n",
      "epoch: 4, batch_id: 120, loss is: 1.3651\n",
      "epoch: 4, batch_id: 140, loss is: 0.2528\n",
      "epoch: 4, batch_id: 160, loss is: 0.0564\n",
      "epoch: 4, batch_id: 180, loss is: 0.0467\n",
      "epoch: 4, batch_id: 200, loss is: 0.5916\n",
      "[validation] accuracy/loss: 0.8825/0.3168\n",
      "epoch: 5, batch_id: 20, loss is: 0.9736\n",
      "epoch: 5, batch_id: 40, loss is: 0.3512\n",
      "epoch: 5, batch_id: 60, loss is: 0.0218\n",
      "epoch: 5, batch_id: 80, loss is: 0.0478\n",
      "epoch: 5, batch_id: 100, loss is: 0.0575\n",
      "epoch: 5, batch_id: 120, loss is: 0.0053\n",
      "epoch: 5, batch_id: 140, loss is: 0.0326\n",
      "epoch: 5, batch_id: 160, loss is: 0.0142\n",
      "epoch: 5, batch_id: 180, loss is: 0.0100\n",
      "epoch: 5, batch_id: 200, loss is: 0.0870\n",
      "[validation] accuracy/loss: 0.9400/0.1825\n"
     ]
    }
   ],
   "source": [
    "opt = paddle.optimizer.Momentum(learning_rate=0.001, momentum=0.9, parameters=model.parameters())\n",
    "\n",
    "# 启动训练过程\n",
    "train_pm(model, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T07:43:30.484549Z",
     "iopub.status.busy": "2023-02-03T07:43:30.483582Z",
     "iopub.status.idle": "2023-02-03T07:43:30.489991Z",
     "shell.execute_reply": "2023-02-03T07:43:30.489310Z",
     "shell.execute_reply.started": "2023-02-03T07:43:30.484513Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(model, dataloader):\n",
    "    model.eval()\n",
    "    pred_list = np.array([])\n",
    "    file_list = np.array([])\n",
    "    for idx, data in enumerate(dataloader):\n",
    "        print('\\r{}/{}'.format(1+idx,len(dataloader)),end='')\n",
    "        x_data, filename = data\n",
    "        img = paddle.to_tensor(x_data)\n",
    "        # 运行模型前向计算，得到预测值\n",
    "        logits = model(img)\n",
    "        # 二分类，sigmoid计算后的结果以0.5为阈值分两个类别\n",
    "        # 计算sigmoid后的预测概率，进行loss计算\n",
    "        pred = F.sigmoid(logits)\n",
    "        pred_list = np.append(pred_list, pred.numpy().ravel())\n",
    "        file_list = np.append(file_list, filename)\n",
    "    return pred_list, file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T07:43:33.244208Z",
     "iopub.status.busy": "2023-02-03T07:43:33.243257Z",
     "iopub.status.idle": "2023-02-03T07:44:11.087254Z",
     "shell.execute_reply": "2023-02-03T07:44:11.086247Z",
     "shell.execute_reply.started": "2023-02-03T07:43:33.244172Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100"
     ]
    }
   ],
   "source": [
    "# 读取模型\n",
    "model = GoogLeNet()\n",
    "model.set_state_dict(paddle.load('model/palmp.pdparams'))\n",
    "testdataset = MyDataset('data/PALM-Testing400',None)\n",
    "testdataloader = paddle.io.DataLoader(testdataset,shuffle=False,drop_last=False,batch_size=BATCH_SIZE)\n",
    "\n",
    "pred_list,file_list = predict(model, testdataloader)\n",
    "pd.DataFrame(np.c_[file_list,pred_list],columns=['FileName','PM Risk']).to_csv('Classification_Results.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ec69b73d0a6985895cc9f85863fd33c600647e90f46968e0f1901acb27b9505b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
