{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T07:29:43.841341Z",
     "iopub.status.busy": "2023-02-03T07:29:43.840427Z",
     "iopub.status.idle": "2023-02-03T07:29:45.916636Z",
     "shell.execute_reply": "2023-02-03T07:29:45.915790Z",
     "shell.execute_reply.started": "2023-02-03T07:29:43.841309Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from paddle import nn\n",
    "from PIL import Image\n",
    "from paddle.distributed import fleet, get_rank\n",
    "from visualdl import LogWriter\n",
    "\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 8\n",
    "EPOCH_NUM = 5\n",
    "logwriter = LogWriter(logdir='./runs')\n",
    "\n",
    "print(paddle.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T07:31:29.272547Z",
     "iopub.status.busy": "2023-02-03T07:31:29.271950Z",
     "iopub.status.idle": "2023-02-03T07:31:29.283702Z",
     "shell.execute_reply": "2023-02-03T07:31:29.282943Z",
     "shell.execute_reply.started": "2023-02-03T07:31:29.272513Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义数据集\n",
    "class MyDataset(paddle.io.Dataset):\n",
    "    def __init__(self, img_dir='data/PALM-Training400/', csv_dir='data/Classification.csv') -> None:\n",
    "        super(MyDataset, self).__init__()\n",
    "        if csv_dir is None:\n",
    "            self.csvfile = None\n",
    "            self.filedir = os.listdir(img_dir)\n",
    "        else:\n",
    "            self.csvfile = pd.read_csv(csv_dir)\n",
    "        self.imgpath = img_dir\n",
    "        pass\n",
    "    def __len__(self):\n",
    "        if self.csvfile is None:\n",
    "            return len(self.filedir)\n",
    "        else:\n",
    "            return len(self.csvfile)\n",
    "        pass\n",
    "    def __getitem__(self, idx):\n",
    "        if self.csvfile is None:\n",
    "            img = np.reshape((np.array(Image.open(self.imgpath+os.sep+self.filedir[idx]).resize((IMAGE_SIZE,IMAGE_SIZE))).astype('float32')),(3,IMAGE_SIZE,IMAGE_SIZE))/256.\n",
    "            lab = self.filedir[idx]\n",
    "        else:\n",
    "            img = np.reshape((np.array(Image.open(self.imgpath+os.sep+self.csvfile['imgName'][idx]).resize((IMAGE_SIZE,IMAGE_SIZE))).astype('float32')),(3,IMAGE_SIZE,IMAGE_SIZE))/256.\n",
    "            lab = np.array(self.csvfile['Label'][idx]).astype('float32')\n",
    "        return img,lab\n",
    "    pass\n",
    "mydataset = MyDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T07:29:47.007713Z",
     "iopub.status.busy": "2023-02-03T07:29:47.006744Z",
     "iopub.status.idle": "2023-02-03T07:29:48.508882Z",
     "shell.execute_reply": "2023-02-03T07:29:48.508026Z",
     "shell.execute_reply.started": "2023-02-03T07:29:47.007677Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0203 16:41:29.185292 212017 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 6.1, Driver API Version: 12.0, Runtime API Version: 11.7\n",
      "W0203 16:41:29.197373 212017 gpu_resources.cc:91] device: 0, cuDNN Version: 8.7.\n"
     ]
    }
   ],
   "source": [
    "# GoogLeNet模型代码 https://www.paddlepaddle.org.cn/tutorials/projectdetail/4464926#anchor-9\n",
    "import numpy as np\n",
    "import paddle\n",
    "from paddle.nn import Conv2D, MaxPool2D, AdaptiveAvgPool2D, Linear\n",
    "## 组网\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "# 定义Inception块\n",
    "class Inception(paddle.nn.Layer):\n",
    "    def __init__(self, c0, c1, c2, c3, c4, **kwargs):\n",
    "        '''\n",
    "        Inception模块的实现代码，\n",
    "        \n",
    "        c1,图(b)中第一条支路1x1卷积的输出通道数，数据类型是整数\n",
    "        c2,图(b)中第二条支路卷积的输出通道数，数据类型是tuple或list, \n",
    "               其中c2[0]是1x1卷积的输出通道数，c2[1]是3x3\n",
    "        c3,图(b)中第三条支路卷积的输出通道数，数据类型是tuple或list, \n",
    "               其中c3[0]是1x1卷积的输出通道数，c3[1]是3x3\n",
    "        c4,图(b)中第一条支路1x1卷积的输出通道数，数据类型是整数\n",
    "        '''\n",
    "        super(Inception, self).__init__()\n",
    "        # 依次创建Inception块每条支路上使用到的操作\n",
    "        self.p1_1 = Conv2D(in_channels=c0,out_channels=c1, kernel_size=1, stride=1)\n",
    "        self.p2_1 = Conv2D(in_channels=c0,out_channels=c2[0], kernel_size=1, stride=1)\n",
    "        self.p2_2 = Conv2D(in_channels=c2[0],out_channels=c2[1], kernel_size=3, padding=1, stride=1)\n",
    "        self.p3_1 = Conv2D(in_channels=c0,out_channels=c3[0], kernel_size=1, stride=1)\n",
    "        self.p3_2 = Conv2D(in_channels=c3[0],out_channels=c3[1], kernel_size=5, padding=2, stride=1)\n",
    "        self.p4_1 = MaxPool2D(kernel_size=3, stride=1, padding=1)\n",
    "        self.p4_2 = Conv2D(in_channels=c0,out_channels=c4, kernel_size=1, stride=1)\n",
    "        \n",
    "        # # 新加一层batchnorm稳定收敛\n",
    "        # self.batchnorm = paddle.nn.BatchNorm2D(c1+c2[1]+c3[1]+c4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 支路1只包含一个1x1卷积\n",
    "        p1 = F.relu(self.p1_1(x))\n",
    "        # 支路2包含 1x1卷积 + 3x3卷积\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        # 支路3包含 1x1卷积 + 5x5卷积\n",
    "        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        # 支路4包含 最大池化和1x1卷积\n",
    "        p4 = F.relu(self.p4_2(self.p4_1(x)))\n",
    "        # 将每个支路的输出特征图拼接在一起作为最终的输出结果\n",
    "        return paddle.concat([p1, p2, p3, p4], axis=1)\n",
    "        # return self.batchnorm()\n",
    "    \n",
    "class GoogLeNet(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(GoogLeNet, self).__init__()\n",
    "        # GoogLeNet包含五个模块，每个模块后面紧跟一个池化层\n",
    "        # 第一个模块包含1个卷积层\n",
    "        self.conv1 = Conv2D(in_channels=3,out_channels=64, kernel_size=7, padding=3, stride=1)\n",
    "        # 3x3最大池化\n",
    "        self.pool1 = MaxPool2D(kernel_size=3, stride=2, padding=1)\n",
    "        # 第二个模块包含2个卷积层\n",
    "        self.conv2_1 = Conv2D(in_channels=64,out_channels=64, kernel_size=1, stride=1)\n",
    "        self.conv2_2 = Conv2D(in_channels=64,out_channels=192, kernel_size=3, padding=1, stride=1)\n",
    "        # 3x3最大池化\n",
    "        self.pool2 = MaxPool2D(kernel_size=3, stride=2, padding=1)\n",
    "        # 第三个模块包含2个Inception块\n",
    "        self.block3_1 = Inception(192, 64, (96, 128), (16, 32), 32)\n",
    "        self.block3_2 = Inception(256, 128, (128, 192), (32, 96), 64)\n",
    "        # 3x3最大池化\n",
    "        self.pool3 = MaxPool2D(kernel_size=3, stride=2, padding=1)\n",
    "        # 第四个模块包含5个Inception块\n",
    "        self.block4_1 = Inception(480, 192, (96, 208), (16, 48), 64)\n",
    "        self.block4_2 = Inception(512, 160, (112, 224), (24, 64), 64)\n",
    "        self.block4_3 = Inception(512, 128, (128, 256), (24, 64), 64)\n",
    "        self.block4_4 = Inception(512, 112, (144, 288), (32, 64), 64)\n",
    "        self.block4_5 = Inception(528, 256, (160, 320), (32, 128), 128)\n",
    "        # 3x3最大池化\n",
    "        self.pool4 = MaxPool2D(kernel_size=3, stride=2, padding=1)\n",
    "        # 第五个模块包含2个Inception块\n",
    "        self.block5_1 = Inception(832, 256, (160, 320), (32, 128), 128)\n",
    "        self.block5_2 = Inception(832, 384, (192, 384), (48, 128), 128)\n",
    "        # 全局池化，用的是global_pooling，不需要设置pool_stride\n",
    "        self.pool5 = AdaptiveAvgPool2D(output_size=1)\n",
    "        self.fc = Linear(in_features=1024, out_features=1)\n",
    "        pass\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2_2(F.relu(self.conv2_1(x)))))\n",
    "        x = self.pool3(self.block3_2(self.block3_1(x)))\n",
    "        x = self.block4_3(self.block4_2(self.block4_1(x)))\n",
    "        x = self.pool4(self.block4_5(self.block4_4(x)))\n",
    "        x = self.pool5(self.block5_2(self.block5_1(x)))\n",
    "        x = paddle.reshape(x, [x.shape[0], -1])\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    pass\n",
    "model = GoogLeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T06:55:36.679232Z",
     "iopub.status.busy": "2023-02-03T06:55:36.678631Z",
     "iopub.status.idle": "2023-02-03T06:55:36.691230Z",
     "shell.execute_reply": "2023-02-03T06:55:36.690326Z",
     "shell.execute_reply.started": "2023-02-03T06:55:36.679188Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_pm(model, optimizer):\n",
    "    # 开启0号GPU训练\n",
    "    paddle.device.set_device('gpu:0')\n",
    "\n",
    "    print('start training ... ')\n",
    "    model.train()\n",
    "    # 定义数据读取器，训练数据读取器和验证数据读取器\n",
    "    train_loader = paddle.io.DataLoader(mydataset,batch_size=BATCH_SIZE,shuffle=True,drop_last=True)\n",
    "    valid_loader = paddle.io.DataLoader(mydataset,batch_size=BATCH_SIZE,shuffle=True,drop_last=True)\n",
    "    for epoch in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            x_data, y_data = data\n",
    "            img = paddle.to_tensor(x_data)\n",
    "            label = paddle.reshape(paddle.to_tensor(y_data),(-1,1))\n",
    "            # 运行模型前向计算，得到预测值\n",
    "            logits = model(img)\n",
    "            loss = F.binary_cross_entropy_with_logits(logits, label)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "\n",
    "            if batch_id % 20 == 19:\n",
    "                print(\"epoch: {}, batch_id: {}, loss is: {:.4f}\".format(epoch+1, batch_id, float(avg_loss.numpy())))\n",
    "            # 反向传播，更新权重，清除梯度\n",
    "            avg_loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.clear_grad()\n",
    "\n",
    "        model.eval()\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        for batch_id, data in enumerate(valid_loader()):\n",
    "            x_data, y_data = data\n",
    "            img = paddle.to_tensor(x_data)\n",
    "            label = paddle.reshape(paddle.to_tensor(y_data),(-1,1))\n",
    "            # 运行模型前向计算，得到预测值\n",
    "            logits = model(img)\n",
    "            # 二分类，sigmoid计算后的结果以0.5为阈值分两个类别\n",
    "            # 计算sigmoid后的预测概率，进行loss计算\n",
    "            pred = F.sigmoid(logits)\n",
    "            loss = F.binary_cross_entropy_with_logits(logits, label)\n",
    "            # 计算预测概率小于0.5的类别\n",
    "            pred2 = pred * (-1.0) + 1.0\n",
    "            # 得到两个类别的预测概率，并沿第一个维度级联\n",
    "            pred = paddle.concat([pred2, pred], axis=1)\n",
    "            acc = paddle.metric.accuracy(pred, paddle.cast(label, dtype='int64'))\n",
    "\n",
    "            accuracies.append(acc.numpy())\n",
    "            losses.append(loss.numpy())\n",
    "        print(\"[validation] accuracy/loss: {:.4f}/{:.4f}\".format(np.mean(accuracies), np.mean(losses)))\n",
    "        model.train()\n",
    "\n",
    "        paddle.save(model.state_dict(), 'model/palmp.pdparams')\n",
    "        paddle.save(optimizer.state_dict(), 'model/palmp.pdopt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-02-03T06:55:36.694649Z",
     "iopub.status.busy": "2023-02-03T06:55:36.694177Z",
     "iopub.status.idle": "2023-02-03T07:13:43.621110Z",
     "shell.execute_reply": "2023-02-03T07:13:43.616991Z",
     "shell.execute_reply.started": "2023-02-03T06:55:36.694606Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training ... \n",
      "epoch: 1, batch_id: 19, loss is: 0.5642\n",
      "epoch: 1, batch_id: 39, loss is: 0.5338\n"
     ]
    }
   ],
   "source": [
    "opt = paddle.optimizer.Momentum(learning_rate=0.001, momentum=0.9, parameters=model.parameters())\n",
    "\n",
    "# 启动训练过程\n",
    "train_pm(model, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T07:43:30.484549Z",
     "iopub.status.busy": "2023-02-03T07:43:30.483582Z",
     "iopub.status.idle": "2023-02-03T07:43:30.489991Z",
     "shell.execute_reply": "2023-02-03T07:43:30.489310Z",
     "shell.execute_reply.started": "2023-02-03T07:43:30.484513Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(model, dataloader):\n",
    "    model.eval()\n",
    "    pred_list = np.array([])\n",
    "    file_list = np.array([])\n",
    "    for idx, data in enumerate(dataloader):\n",
    "        print('\\r{}/{}'.format(1+idx,len(dataloader)),end='')\n",
    "        x_data, filename = data\n",
    "        img = paddle.to_tensor(x_data)\n",
    "        # 运行模型前向计算，得到预测值\n",
    "        logits = model(img)\n",
    "        # 二分类，sigmoid计算后的结果以0.5为阈值分两个类别\n",
    "        # 计算sigmoid后的预测概率，进行loss计算\n",
    "        pred = F.sigmoid(logits)\n",
    "        pred_list = np.append(pred_list, pred.numpy().ravel())\n",
    "        file_list = np.append(file_list, filename)\n",
    "    return pred_list, file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T07:43:33.244208Z",
     "iopub.status.busy": "2023-02-03T07:43:33.243257Z",
     "iopub.status.idle": "2023-02-03T07:44:11.087254Z",
     "shell.execute_reply": "2023-02-03T07:44:11.086247Z",
     "shell.execute_reply.started": "2023-02-03T07:43:33.244172Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13"
     ]
    }
   ],
   "source": [
    "# 读取模型\n",
    "model =VGG([[2,2,3,3,3], [[3,64],[64,128],[128,256],[256,512],[512,512]]])\n",
    "model.set_state_dict(paddle.load('model/palmp.pdparams'))\n",
    "testdataset = MyDataset('PALM-Testing400-Images',None)\n",
    "testdataloader = paddle.io.DataLoader(testdataset,shuffle=False,drop_last=False,batch_size=BATCH_SIZE)\n",
    "\n",
    "pred_list,file_list = predict(model, testdataloader)\n",
    "pd.DataFrame(np.c_[file_list,pred_list],columns=['FileName','PM Risk']).to_csv('Classification_Results.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ec69b73d0a6985895cc9f85863fd33c600647e90f46968e0f1901acb27b9505b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
